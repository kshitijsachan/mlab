{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import einsum\n",
    "from einops import rearrange, reduce, repeat\n",
    "import bert_tests\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention pattern raw MATCH!!!!!!!!\n",
      " SHAPE (2, 12, 3, 3) MEAN: 0.005254 STD: 0.106 VALS [-0.05107 -0.06011 0.043 0.01762 -0.05089 0.07877 0.2465 0.2506 0.02985 0.01162...]\n"
     ]
    }
   ],
   "source": [
    "def raw_attention_pattern(\n",
    "        token_activations,  # Tensor[batch_size, seq_length, hidden_size(768)],\n",
    "        num_heads,\n",
    "        project_query,      # nn.Module, (Tensor[..., 768]) -> Tensor[..., 768],\n",
    "        project_key,        # nn.Module, (Tensor[..., 768]) -> Tensor[..., 768]\n",
    "): # -> Tensor[batch_size, head_num, key_token: seq_length, query_token: seq_length]:\n",
    "    Q = project_query(token_activations)\n",
    "    K = project_key(token_activations)\n",
    "    Q = rearrange(Q, 'b seqlen (headnum headsize) -> b headnum seqlen headsize', headnum=num_heads)\n",
    "    K = rearrange(K, 'b seqlen (headnum headsize) -> b headnum seqlen headsize', headnum=num_heads)\n",
    "    headsize = K.shape[-1]\n",
    "    dot_prod = einsum('bhql,bhkl-> bhkq', Q, K) / math.sqrt(headsize)\n",
    "    return dot_prod\n",
    "\n",
    "bert_tests.test_attention_pattern_fn(raw_attention_pattern)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.003646 STD: 0.1161 VALS [0.01265 0.2171 -0.1186 0.001848 0.01751 -0.2293 -0.03623 0.01134 0.09451 -0.05312...]\n"
     ]
    }
   ],
   "source": [
    "def bert_attention(\n",
    "        token_activations, #: Tensor[batch_size, seq_length, hidden_size (768)],\n",
    "        num_heads: int,\n",
    "        attention_pattern, #: Tensor[batch_size,num_heads, seq_length, seq_length],\n",
    "        project_value, #: function( (Tensor[..., 768]) -> Tensor[..., 768] ),\n",
    "        project_output, #: function( (Tensor[..., 768]) -> Tensor[..., 768] )\n",
    "): # -> Tensor[batch_size, seq_length, hidden_size]\n",
    "    attention_pattern = attention_pattern.softmax(dim=2)\n",
    "    V = project_value(token_activations)\n",
    "    V = rearrange(V, 'b seqlen (headnum headsize) -> b headnum seqlen headsize', headnum=num_heads)\n",
    "    attention = einsum('bhkq,bhkl->bhql', attention_pattern, V)\n",
    "    attention = rearrange(attention, 'b headnum seqlen hiddensize -> b seqlen (headnum hiddensize)')\n",
    "    ans = project_output(attention)\n",
    "    return ans\n",
    "bert_tests.test_attention_fn(bert_attention)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768]) torch.Size([2, 3, 768])\n",
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.001554 STD: 0.1736 VALS [-0.08316 -0.09165 -0.03188 -0.03013 0.1001 0.09549 -0.1046 0.07742 0.0424 0.05553...]\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadedSelfAttention(t.nn.Module):\n",
    "    def __init__(self, num_heads: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.project_query = t.nn.Linear(hidden_size, hidden_size)\n",
    "        self.project_key = t.nn.Linear(hidden_size, hidden_size)\n",
    "        self.project_value = t.nn.Linear(hidden_size, hidden_size)\n",
    "        self.project_output = t.nn.Linear(hidden_size, hidden_size)\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x): # Tensor[batch_size, seq_length, hidden_size]\n",
    "        raw_attention = raw_attention_pattern(x, self.num_heads, self.project_query, self.project_key)\n",
    "        attention = bert_attention(x, self.num_heads, raw_attention, self.project_value, self.project_output)\n",
    "        return attention\n",
    "\n",
    "bert_tests.test_bert_attention(MultiHeadedSelfAttention)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert mlp MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.003054 STD: 0.1041 VALS [0.1262 0.01134 0.06912 0.05845 0.06832 0.06498 -0.07017 -0.1155 -0.004871 0.2145...]\n"
     ]
    }
   ],
   "source": [
    "def bert_mlp(token_activations, #: torch.Tensor[batch_size,seq_length,768],\n",
    "             linear_1: t.nn.Module, linear_2: t.nn.Module\n",
    "             ): #-> torch.Tensor[batch_size, seq_length, 768]\n",
    "    return linear_2(t.nn.functional.gelu(linear_1(token_activations)))\n",
    "\n",
    "bert_tests.test_bert_mlp(bert_mlp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "class BertMLP(t.nn.Module):\n",
    "    def __init__(self, input_size: int, intermediate_size: int):\n",
    "        super().__init__()\n",
    "        self.linear1 = t.nn.Linear(input_size, intermediate_size)\n",
    "        self.linear2 = t.nn.Linear(intermediate_size, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return bert_mlp(x, self.linear1, self.linear2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer norm MATCH!!!!!!!!\n",
      " SHAPE (20, 10) MEAN: -5.96e-10 STD: 1.003 VALS [-1.876 0.9704 -0.2068 0.07342 0.6658 1.202 -0.8645 0.569 -1.36 0.8267...]\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(t.nn.Module):\n",
    "    def __init__(self, normalized_dim: int):\n",
    "        super().__init__()\n",
    "        self.weight = t.nn.Parameter(t.ones(normalized_dim))\n",
    "        self.bias = t.nn.Parameter(t.zeros(normalized_dim))\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input - input.mean(dim=-1, keepdim=True).detach()\n",
    "        input = input / input.std(dim=-1, keepdim=True, unbiased=False)\n",
    "        return input * self.weight + self.bias\n",
    "bert_tests.test_layer_norm(LayerNorm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -4.139e-10 STD: 1 VALS [0.007132 -0.04372 0.6502 -0.5972 -1.097 0.7267 0.1275 -0.6035 -0.2226 0.2145...]\n"
     ]
    }
   ],
   "source": [
    "class ResNet(t.nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.m = module\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.m(x) + x\n",
    "\n",
    "class BertBlock(t.nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 intermediate_size: int,\n",
    "                 num_heads: int,\n",
    "                 dropout: float):\n",
    "        super().__init__()\n",
    "        self.layers = t.nn.Sequential(\n",
    "            ResNet(MultiHeadedSelfAttention(num_heads, hidden_size)),\n",
    "            LayerNorm(hidden_size),\n",
    "            ResNet(t.nn.Sequential(BertMLP(hidden_size, intermediate_size), t.nn.Dropout(dropout))),\n",
    "            LayerNorm(hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "bert_tests.test_bert_block(BertBlock)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 5) MEAN: -0.2095 STD: 0.8819 VALS [-0.8435 0.0199 -0.7648 1.023 -1.396 -0.8435 0.0199 -0.7648 1.023 -1.396...]\n"
     ]
    }
   ],
   "source": [
    "class Embedding(t.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.embedding = t.nn.Parameter(t.randn((vocab_size, embed_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding[x]\n",
    "\n",
    "bert_tests.test_embedding(Embedding)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In addition to embedding the token itself, Bert also explicitly represents the token’s position in the sentence (“positional embedding”) and the token type (“token type embedding”).\n",
    "\n",
    "So in total Bert stores three (learned) embedding matrices:\n",
    "The token embedding matrix [28996, embedding_size]\n",
    "The token_type embedding matrix [2, embedding_size]\n",
    "The positional embedding matrix [512, embedding_size]\n",
    "\n",
    "(Not all transformers learn their positional embeddings, some use hardcoded sinusoidal embeddings.)\n",
    "\n",
    "To represent a token (word or word piece), it:\n",
    "Looks up the input_id in the token_embedding matrix to get a vector [embedding_size]\n",
    "Looks up the token_type of the token in the token_type embedding matrix. The token_type is always either 0 or 1, indicating whether the word belongs to “sentence A” or “sentence B”. This is relevant for tasks like paraphrasing (first phrasing, second phrasing), question answering (question, answer), or similar. For language modelling, this doesn’t matter at all – just assume all tokens have token_type = 0.\n",
    "Looks up the position of the token in the sentence (e.g. “dog” has position 2, “is” has position 3) in the positional embedding matrix. The max sentence length is 512.\n",
    "Adds up the three embeddings and applies layer norm, then dropout.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 8.278e-10 STD: 1 VALS [-0.1558 -0.906 1.358 -0.1096 0.02568 -0.9749 -0.2617 0.05282 -2.021 -0.3563...]\n"
     ]
    }
   ],
   "source": [
    "def bert_embedding(\n",
    "        input_ids, # [batch, seqlen]\n",
    "        token_type_ids, # [batch, seqlen]\n",
    "        position_embedding: Embedding,\n",
    "        token_embedding: Embedding,\n",
    "        token_type_embedding: Embedding,\n",
    "        layer_norm: LayerNorm,\n",
    "        dropout: t.nn.Dropout):\n",
    "\n",
    "    word_embeddings = token_embedding(input_ids) # [batch, embedding_size]\n",
    "    type_embeddings = token_type_embedding(token_type_ids)\n",
    "    positions = repeat(t.arange(0, input_ids.shape[-1], device=token_type_ids.device), 'n -> b n', b=input_ids.shape[0])\n",
    "    position_embeddings = position_embedding(t.squeeze(positions))\n",
    "    # summed_embedding = einsum('bse,bse,bse->bse', word_embeddings, type_embeddings, position_embeddings)\n",
    "    summed_embedding = word_embeddings + type_embeddings + position_embeddings\n",
    "    return dropout(layer_norm(summed_embedding))\n",
    "\n",
    "bert_tests.test_bert_embedding_fn(bert_embedding)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "max_position_embeddings is the maximum sentence length, 512 above\n",
    "type_vocab_size is the number of token_types, 2 above\n",
    "Use your Embedding to store the embedding matrices.\n",
    "Initialise the embeddings in the order token, position, token_type.\n",
    "Test your module with bert_tests.test_bert_embedding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -7.14e-09 STD: 1 VALS [-1.084 -0.9279 0.962 -0.7941 -2.865 1.584 0.5704 1.182 0.2184 0.7135...]\n"
     ]
    }
   ],
   "source": [
    "class BertEmbedding(t.nn.Module):\n",
    "    def __init__(self, vocab_size: int, hidden_size: int, max_position_embeddings: int, type_vocab_size: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.token_embedding = Embedding(vocab_size, hidden_size)\n",
    "        self.position_embedding = Embedding(max_position_embeddings, hidden_size)\n",
    "        self.token_type_embedding = Embedding(type_vocab_size, hidden_size)\n",
    "        self.layernorm = LayerNorm(hidden_size)\n",
    "        self.dropout = t.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        return bert_embedding(input_ids, token_type_ids, self.position_embedding, self.token_embedding, self.token_type_embedding, self.layernorm, self.dropout)\n",
    "\n",
    "bert_tests.test_bert_embedding(BertEmbedding)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make a nn.Module called Bert, with the following methods:\n",
    "__init__(\n",
    "self, vocab_size: int, hidden_size: int,\n",
    "max_position_embeddings: int, type_vocab_size: int,\n",
    "dropout: float, intermediate_size: int, num_heads: int,\n",
    "num_layers: int\n",
    ")\n",
    "forward(self, input_ids)\n",
    "\n",
    "- hidden_size is the embedding size, 768\n",
    "- dropout probability is the same for both the embedding and the BertMLP\n",
    "- num_layers is the number of BertBlocks\n",
    "- Assume token_type_ids are zero. Make sure your code is GPU-friendly, e.g. place token_type_ids on the same device as input_ids.\n",
    "- Use the architecture diagram on page 1 to guide you.\n",
    "- In addition to the N encoding blocks, we will add a Linear layer [hidden_size, hidden_size], GELU, LayerNorm, and an unembedding layer (Linear[hidden_size, vocab_size]) at the end.\n",
    "- If you think the whole thing works, test it with bert_tests.test_bert.\n",
    "\n",
    "Note: don’t actually compute the softmax at the end. This allows you to use log_softmax later instead, which preserves the differences between very small probabilities, such as 1e-30 and 1e-35 which would be destroyed by limited precision under a softmax."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 768]) torch.Size([1, 4, 768])\n",
      "torch.Size([1, 4, 768]) torch.Size([1, 4, 768])\n",
      "torch.Size([1, 4, 768]) torch.Size([1, 4, 768])\n",
      "torch.Size([1, 4, 768]) torch.Size([1, 4, 768])\n",
      "torch.Size([1, 4, 768]) torch.Size([1, 4, 768])\n",
      "torch.Size([1, 4, 768]) torch.Size([1, 4, 768])\n",
      "torch.Size([1, 4, 768]) torch.Size([1, 4, 768])\n",
      "torch.Size([1, 4, 768]) torch.Size([1, 4, 768])\n",
      "torch.Size([1, 4, 768]) torch.Size([1, 4, 768])\n",
      "torch.Size([1, 4, 768]) torch.Size([1, 4, 768])\n",
      "torch.Size([1, 4, 768]) torch.Size([1, 4, 768])\n",
      "torch.Size([1, 4, 768]) torch.Size([1, 4, 768])\n",
      "torch.Size([1, 4, 768]) torch.Size([1, 4, 28996])\n",
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 4, 28996) MEAN: 0.003031 STD: 0.5765 VALS [-0.5742 -0.4321 0.1186 -0.7165 -0.5262 0.4967 1.223 0.3165 -0.3247 -0.5717...]\n"
     ]
    }
   ],
   "source": [
    "class Bert(t.nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size: int,\n",
    "                 hidden_size: int,\n",
    "                 max_position_embeddings: int,\n",
    "                 type_vocab_size: int,\n",
    "                 dropout: float,\n",
    "                 intermediate_size: int,\n",
    "                 num_heads: int,\n",
    "                 num_layers: int):\n",
    "        super().__init__()\n",
    "        self.embed = BertEmbedding(vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout)\n",
    "        self.layers = t.nn.Sequential(\n",
    "            *[BertBlock(hidden_size, intermediate_size, num_heads, dropout) for _ in range(num_layers)],\n",
    "            t.nn.Linear(hidden_size, hidden_size),\n",
    "            t.nn.GELU(),\n",
    "            LayerNorm(hidden_size),\n",
    "            t.nn.Linear(hidden_size, vocab_size)\n",
    "            )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        token_type_ids = t.zeros_like(input_ids, device=input_ids.device)\n",
    "        ans = self.embed(input_ids, token_type_ids)\n",
    "        ans2 = self.layers(ans)\n",
    "        print(ans.shape, ans2.shape)\n",
    "        return ans2\n",
    "\n",
    "bert_tests.test_bert(Bert)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.token_embedding.weight embed.token_embedding.embedding embedding.token_embedding.weight\n",
      "embedding.position_embedding.weight embed.position_embedding.embedding embedding.position_embedding.weight\n",
      "embedding.token_type_embedding.weight embed.token_type_embedding.embedding embedding.token_type_embedding.weight\n",
      "embedding.layer_norm.weight embed.layernorm.weight embed.layernorm.\u0000\n",
      "embedding.layer_norm.bias embed.layernorm.bias embed.layernorm.\u0000\n",
      "transformer.0.layer_norm.weight layers.0.layers.0.m.project_query.weight transformer.0.layer_norm.weight\n",
      "transformer.0.layer_norm.bias layers.0.layers.0.m.project_query.bias transformer.0.layer_norm.bias\n",
      "transformer.0.attention.pattern.project_query.weight layers.0.layers.0.m.project_key.weight transformer.0.attention.pattern.project_query.weight\n",
      "transformer.0.attention.pattern.project_query.bias layers.0.layers.0.m.project_key.bias transformer.0.attention.pattern.project_query.bias\n",
      "transformer.0.attention.pattern.project_key.weight layers.0.layers.0.m.project_value.weight transformer.0.attention.pattern.project_key.weight\n",
      "transformer.0.attention.pattern.project_key.bias layers.0.layers.0.m.project_value.bias transformer.0.attention.pattern.project_key.bias\n",
      "transformer.0.attention.project_value.weight layers.0.layers.0.m.project_output.weight transformer.0.attention.project_value.weight\n",
      "transformer.0.attention.project_value.bias layers.0.layers.0.m.project_output.bias transformer.0.attention.project_value.bias\n",
      "transformer.0.attention.project_out.weight layers.0.layers.1.weight transformer.0.attention.project_out.weight\n",
      "transformer.0.attention.project_out.bias layers.0.layers.1.bias transformer.0.attention.project_out.bias\n",
      "transformer.0.residual.mlp1.weight layers.0.layers.2.m.0.linear1.weight transformer.0.residual.mlp1.weight\n",
      "transformer.0.residual.mlp1.bias layers.0.layers.2.m.0.linear1.bias transformer.0.residual.mlp1.bias\n",
      "transformer.0.residual.mlp2.weight layers.0.layers.2.m.0.linear2.weight transformer.0.residual.mlp2.weight\n",
      "transformer.0.residual.mlp2.bias layers.0.layers.2.m.0.linear2.bias transformer.0.residual.mlp2.bias\n",
      "transformer.0.residual.layer_norm.weight layers.0.layers.3.weight transformer.0.residual.layer_norm.weight\n",
      "transformer.0.residual.layer_norm.bias layers.0.layers.3.bias transformer.0.residual.layer_norm.bias\n",
      "transformer.1.layer_norm.weight layers.1.layers.0.m.project_query.weight transformer.1.layer_norm.weight\n",
      "transformer.1.layer_norm.bias layers.1.layers.0.m.project_query.bias transformer.1.layer_norm.bias\n",
      "transformer.1.attention.pattern.project_query.weight layers.1.layers.0.m.project_key.weight transformer.1.attention.pattern.project_query.weight\n",
      "transformer.1.attention.pattern.project_query.bias layers.1.layers.0.m.project_key.bias transformer.1.attention.pattern.project_query.bias\n",
      "transformer.1.attention.pattern.project_key.weight layers.1.layers.0.m.project_value.weight transformer.1.attention.pattern.project_key.weight\n",
      "transformer.1.attention.pattern.project_key.bias layers.1.layers.0.m.project_value.bias transformer.1.attention.pattern.project_key.bias\n",
      "transformer.1.attention.project_value.weight layers.1.layers.0.m.project_output.weight transformer.1.attention.project_value.weight\n",
      "transformer.1.attention.project_value.bias layers.1.layers.0.m.project_output.bias transformer.1.attention.project_value.bias\n",
      "transformer.1.attention.project_out.weight layers.1.layers.1.weight transformer.1.attention.project_out.weight\n",
      "transformer.1.attention.project_out.bias layers.1.layers.1.bias transformer.1.attention.project_out.bias\n",
      "transformer.1.residual.mlp1.weight layers.1.layers.2.m.0.linear1.weight transformer.1.residual.mlp1.weight\n",
      "transformer.1.residual.mlp1.bias layers.1.layers.2.m.0.linear1.bias transformer.1.residual.mlp1.bias\n",
      "transformer.1.residual.mlp2.weight layers.1.layers.2.m.0.linear2.weight transformer.1.residual.mlp2.weight\n",
      "transformer.1.residual.mlp2.bias layers.1.layers.2.m.0.linear2.bias transformer.1.residual.mlp2.bias\n",
      "transformer.1.residual.layer_norm.weight layers.1.layers.3.weight transformer.1.residual.layer_norm.weight\n",
      "transformer.1.residual.layer_norm.bias layers.1.layers.3.bias transformer.1.residual.layer_norm.bias\n",
      "transformer.2.layer_norm.weight layers.2.layers.0.m.project_query.weight transformer.2.layer_norm.weight\n",
      "transformer.2.layer_norm.bias layers.2.layers.0.m.project_query.bias transformer.2.layer_norm.bias\n",
      "transformer.2.attention.pattern.project_query.weight layers.2.layers.0.m.project_key.weight transformer.2.attention.pattern.project_query.weight\n",
      "transformer.2.attention.pattern.project_query.bias layers.2.layers.0.m.project_key.bias transformer.2.attention.pattern.project_query.bias\n",
      "transformer.2.attention.pattern.project_key.weight layers.2.layers.0.m.project_value.weight transformer.2.attention.pattern.project_key.weight\n",
      "transformer.2.attention.pattern.project_key.bias layers.2.layers.0.m.project_value.bias transformer.2.attention.pattern.project_key.bias\n",
      "transformer.2.attention.project_value.weight layers.2.layers.0.m.project_output.weight transformer.2.attention.project_value.weight\n",
      "transformer.2.attention.project_value.bias layers.2.layers.0.m.project_output.bias transformer.2.attention.project_value.bias\n",
      "transformer.2.attention.project_out.weight layers.2.layers.1.weight transformer.2.attention.project_out.weight\n",
      "transformer.2.attention.project_out.bias layers.2.layers.1.bias transformer.2.attention.project_out.bias\n",
      "transformer.2.residual.mlp1.weight layers.2.layers.2.m.0.linear1.weight transformer.2.residual.mlp1.weight\n",
      "transformer.2.residual.mlp1.bias layers.2.layers.2.m.0.linear1.bias transformer.2.residual.mlp1.bias\n",
      "transformer.2.residual.mlp2.weight layers.2.layers.2.m.0.linear2.weight transformer.2.residual.mlp2.weight\n",
      "transformer.2.residual.mlp2.bias layers.2.layers.2.m.0.linear2.bias transformer.2.residual.mlp2.bias\n",
      "transformer.2.residual.layer_norm.weight layers.2.layers.3.weight transformer.2.residual.layer_norm.weight\n",
      "transformer.2.residual.layer_norm.bias layers.2.layers.3.bias transformer.2.residual.layer_norm.bias\n",
      "transformer.3.layer_norm.weight layers.3.layers.0.m.project_query.weight transformer.3.layer_norm.weight\n",
      "transformer.3.layer_norm.bias layers.3.layers.0.m.project_query.bias transformer.3.layer_norm.bias\n",
      "transformer.3.attention.pattern.project_query.weight layers.3.layers.0.m.project_key.weight transformer.3.attention.pattern.project_query.weight\n",
      "transformer.3.attention.pattern.project_query.bias layers.3.layers.0.m.project_key.bias transformer.3.attention.pattern.project_query.bias\n",
      "transformer.3.attention.pattern.project_key.weight layers.3.layers.0.m.project_value.weight transformer.3.attention.pattern.project_key.weight\n",
      "transformer.3.attention.pattern.project_key.bias layers.3.layers.0.m.project_value.bias transformer.3.attention.pattern.project_key.bias\n",
      "transformer.3.attention.project_value.weight layers.3.layers.0.m.project_output.weight transformer.3.attention.project_value.weight\n",
      "transformer.3.attention.project_value.bias layers.3.layers.0.m.project_output.bias transformer.3.attention.project_value.bias\n",
      "transformer.3.attention.project_out.weight layers.3.layers.1.weight transformer.3.attention.project_out.weight\n",
      "transformer.3.attention.project_out.bias layers.3.layers.1.bias transformer.3.attention.project_out.bias\n",
      "transformer.3.residual.mlp1.weight layers.3.layers.2.m.0.linear1.weight transformer.3.residual.mlp1.weight\n",
      "transformer.3.residual.mlp1.bias layers.3.layers.2.m.0.linear1.bias transformer.3.residual.mlp1.bias\n",
      "transformer.3.residual.mlp2.weight layers.3.layers.2.m.0.linear2.weight transformer.3.residual.mlp2.weight\n",
      "transformer.3.residual.mlp2.bias layers.3.layers.2.m.0.linear2.bias transformer.3.residual.mlp2.bias\n",
      "transformer.3.residual.layer_norm.weight layers.3.layers.3.weight transformer.3.residual.layer_norm.weight\n",
      "transformer.3.residual.layer_norm.bias layers.3.layers.3.bias transformer.3.residual.layer_norm.bias\n",
      "transformer.4.layer_norm.weight layers.4.layers.0.m.project_query.weight transformer.4.layer_norm.weight\n",
      "transformer.4.layer_norm.bias layers.4.layers.0.m.project_query.bias transformer.4.layer_norm.bias\n",
      "transformer.4.attention.pattern.project_query.weight layers.4.layers.0.m.project_key.weight transformer.4.attention.pattern.project_query.weight\n",
      "transformer.4.attention.pattern.project_query.bias layers.4.layers.0.m.project_key.bias transformer.4.attention.pattern.project_query.bias\n",
      "transformer.4.attention.pattern.project_key.weight layers.4.layers.0.m.project_value.weight transformer.4.attention.pattern.project_key.weight\n",
      "transformer.4.attention.pattern.project_key.bias layers.4.layers.0.m.project_value.bias transformer.4.attention.pattern.project_key.bias\n",
      "transformer.4.attention.project_value.weight layers.4.layers.0.m.project_output.weight transformer.4.attention.project_value.weight\n",
      "transformer.4.attention.project_value.bias layers.4.layers.0.m.project_output.bias transformer.4.attention.project_value.bias\n",
      "transformer.4.attention.project_out.weight layers.4.layers.1.weight transformer.4.attention.project_out.weight\n",
      "transformer.4.attention.project_out.bias layers.4.layers.1.bias transformer.4.attention.project_out.bias\n",
      "transformer.4.residual.mlp1.weight layers.4.layers.2.m.0.linear1.weight transformer.4.residual.mlp1.weight\n",
      "transformer.4.residual.mlp1.bias layers.4.layers.2.m.0.linear1.bias transformer.4.residual.mlp1.bias\n",
      "transformer.4.residual.mlp2.weight layers.4.layers.2.m.0.linear2.weight transformer.4.residual.mlp2.weight\n",
      "transformer.4.residual.mlp2.bias layers.4.layers.2.m.0.linear2.bias transformer.4.residual.mlp2.bias\n",
      "transformer.4.residual.layer_norm.weight layers.4.layers.3.weight transformer.4.residual.layer_norm.weight\n",
      "transformer.4.residual.layer_norm.bias layers.4.layers.3.bias transformer.4.residual.layer_norm.bias\n",
      "transformer.5.layer_norm.weight layers.5.layers.0.m.project_query.weight transformer.5.layer_norm.weight\n",
      "transformer.5.layer_norm.bias layers.5.layers.0.m.project_query.bias transformer.5.layer_norm.bias\n",
      "transformer.5.attention.pattern.project_query.weight layers.5.layers.0.m.project_key.weight transformer.5.attention.pattern.project_query.weight\n",
      "transformer.5.attention.pattern.project_query.bias layers.5.layers.0.m.project_key.bias transformer.5.attention.pattern.project_query.bias\n",
      "transformer.5.attention.pattern.project_key.weight layers.5.layers.0.m.project_value.weight transformer.5.attention.pattern.project_key.weight\n",
      "transformer.5.attention.pattern.project_key.bias layers.5.layers.0.m.project_value.bias transformer.5.attention.pattern.project_key.bias\n",
      "transformer.5.attention.project_value.weight layers.5.layers.0.m.project_output.weight transformer.5.attention.project_value.weight\n",
      "transformer.5.attention.project_value.bias layers.5.layers.0.m.project_output.bias transformer.5.attention.project_value.bias\n",
      "transformer.5.attention.project_out.weight layers.5.layers.1.weight transformer.5.attention.project_out.weight\n",
      "transformer.5.attention.project_out.bias layers.5.layers.1.bias transformer.5.attention.project_out.bias\n",
      "transformer.5.residual.mlp1.weight layers.5.layers.2.m.0.linear1.weight transformer.5.residual.mlp1.weight\n",
      "transformer.5.residual.mlp1.bias layers.5.layers.2.m.0.linear1.bias transformer.5.residual.mlp1.bias\n",
      "transformer.5.residual.mlp2.weight layers.5.layers.2.m.0.linear2.weight transformer.5.residual.mlp2.weight\n",
      "transformer.5.residual.mlp2.bias layers.5.layers.2.m.0.linear2.bias transformer.5.residual.mlp2.bias\n",
      "transformer.5.residual.layer_norm.weight layers.5.layers.3.weight transformer.5.residual.layer_norm.weight\n",
      "transformer.5.residual.layer_norm.bias layers.5.layers.3.bias transformer.5.residual.layer_norm.bias\n",
      "transformer.6.layer_norm.weight layers.6.layers.0.m.project_query.weight transformer.6.layer_norm.weight\n",
      "transformer.6.layer_norm.bias layers.6.layers.0.m.project_query.bias transformer.6.layer_norm.bias\n",
      "transformer.6.attention.pattern.project_query.weight layers.6.layers.0.m.project_key.weight transformer.6.attention.pattern.project_query.weight\n",
      "transformer.6.attention.pattern.project_query.bias layers.6.layers.0.m.project_key.bias transformer.6.attention.pattern.project_query.bias\n",
      "transformer.6.attention.pattern.project_key.weight layers.6.layers.0.m.project_value.weight transformer.6.attention.pattern.project_key.weight\n",
      "transformer.6.attention.pattern.project_key.bias layers.6.layers.0.m.project_value.bias transformer.6.attention.pattern.project_key.bias\n",
      "transformer.6.attention.project_value.weight layers.6.layers.0.m.project_output.weight transformer.6.attention.project_value.weight\n",
      "transformer.6.attention.project_value.bias layers.6.layers.0.m.project_output.bias transformer.6.attention.project_value.bias\n",
      "transformer.6.attention.project_out.weight layers.6.layers.1.weight transformer.6.attention.project_out.weight\n",
      "transformer.6.attention.project_out.bias layers.6.layers.1.bias transformer.6.attention.project_out.bias\n",
      "transformer.6.residual.mlp1.weight layers.6.layers.2.m.0.linear1.weight transformer.6.residual.mlp1.weight\n",
      "transformer.6.residual.mlp1.bias layers.6.layers.2.m.0.linear1.bias transformer.6.residual.mlp1.bias\n",
      "transformer.6.residual.mlp2.weight layers.6.layers.2.m.0.linear2.weight transformer.6.residual.mlp2.weight\n",
      "transformer.6.residual.mlp2.bias layers.6.layers.2.m.0.linear2.bias transformer.6.residual.mlp2.bias\n",
      "transformer.6.residual.layer_norm.weight layers.6.layers.3.weight transformer.6.residual.layer_norm.weight\n",
      "transformer.6.residual.layer_norm.bias layers.6.layers.3.bias transformer.6.residual.layer_norm.bias\n",
      "transformer.7.layer_norm.weight layers.7.layers.0.m.project_query.weight transformer.7.layer_norm.weight\n",
      "transformer.7.layer_norm.bias layers.7.layers.0.m.project_query.bias transformer.7.layer_norm.bias\n",
      "transformer.7.attention.pattern.project_query.weight layers.7.layers.0.m.project_key.weight transformer.7.attention.pattern.project_query.weight\n",
      "transformer.7.attention.pattern.project_query.bias layers.7.layers.0.m.project_key.bias transformer.7.attention.pattern.project_query.bias\n",
      "transformer.7.attention.pattern.project_key.weight layers.7.layers.0.m.project_value.weight transformer.7.attention.pattern.project_key.weight\n",
      "transformer.7.attention.pattern.project_key.bias layers.7.layers.0.m.project_value.bias transformer.7.attention.pattern.project_key.bias\n",
      "transformer.7.attention.project_value.weight layers.7.layers.0.m.project_output.weight transformer.7.attention.project_value.weight\n",
      "transformer.7.attention.project_value.bias layers.7.layers.0.m.project_output.bias transformer.7.attention.project_value.bias\n",
      "transformer.7.attention.project_out.weight layers.7.layers.1.weight transformer.7.attention.project_out.weight\n",
      "transformer.7.attention.project_out.bias layers.7.layers.1.bias transformer.7.attention.project_out.bias\n",
      "transformer.7.residual.mlp1.weight layers.7.layers.2.m.0.linear1.weight transformer.7.residual.mlp1.weight\n",
      "transformer.7.residual.mlp1.bias layers.7.layers.2.m.0.linear1.bias transformer.7.residual.mlp1.bias\n",
      "transformer.7.residual.mlp2.weight layers.7.layers.2.m.0.linear2.weight transformer.7.residual.mlp2.weight\n",
      "transformer.7.residual.mlp2.bias layers.7.layers.2.m.0.linear2.bias transformer.7.residual.mlp2.bias\n",
      "transformer.7.residual.layer_norm.weight layers.7.layers.3.weight transformer.7.residual.layer_norm.weight\n",
      "transformer.7.residual.layer_norm.bias layers.7.layers.3.bias transformer.7.residual.layer_norm.bias\n",
      "transformer.8.layer_norm.weight layers.8.layers.0.m.project_query.weight transformer.8.layer_norm.weight\n",
      "transformer.8.layer_norm.bias layers.8.layers.0.m.project_query.bias transformer.8.layer_norm.bias\n",
      "transformer.8.attention.pattern.project_query.weight layers.8.layers.0.m.project_key.weight transformer.8.attention.pattern.project_query.weight\n",
      "transformer.8.attention.pattern.project_query.bias layers.8.layers.0.m.project_key.bias transformer.8.attention.pattern.project_query.bias\n",
      "transformer.8.attention.pattern.project_key.weight layers.8.layers.0.m.project_value.weight transformer.8.attention.pattern.project_key.weight\n",
      "transformer.8.attention.pattern.project_key.bias layers.8.layers.0.m.project_value.bias transformer.8.attention.pattern.project_key.bias\n",
      "transformer.8.attention.project_value.weight layers.8.layers.0.m.project_output.weight transformer.8.attention.project_value.weight\n",
      "transformer.8.attention.project_value.bias layers.8.layers.0.m.project_output.bias transformer.8.attention.project_value.bias\n",
      "transformer.8.attention.project_out.weight layers.8.layers.1.weight transformer.8.attention.project_out.weight\n",
      "transformer.8.attention.project_out.bias layers.8.layers.1.bias transformer.8.attention.project_out.bias\n",
      "transformer.8.residual.mlp1.weight layers.8.layers.2.m.0.linear1.weight transformer.8.residual.mlp1.weight\n",
      "transformer.8.residual.mlp1.bias layers.8.layers.2.m.0.linear1.bias transformer.8.residual.mlp1.bias\n",
      "transformer.8.residual.mlp2.weight layers.8.layers.2.m.0.linear2.weight transformer.8.residual.mlp2.weight\n",
      "transformer.8.residual.mlp2.bias layers.8.layers.2.m.0.linear2.bias transformer.8.residual.mlp2.bias\n",
      "transformer.8.residual.layer_norm.weight layers.8.layers.3.weight transformer.8.residual.layer_norm.weight\n",
      "transformer.8.residual.layer_norm.bias layers.8.layers.3.bias transformer.8.residual.layer_norm.bias\n",
      "transformer.9.layer_norm.weight layers.9.layers.0.m.project_query.weight transformer.9.layer_norm.weight\n",
      "transformer.9.layer_norm.bias layers.9.layers.0.m.project_query.bias transformer.9.layer_norm.bias\n",
      "transformer.9.attention.pattern.project_query.weight layers.9.layers.0.m.project_key.weight transformer.9.attention.pattern.project_query.weight\n",
      "transformer.9.attention.pattern.project_query.bias layers.9.layers.0.m.project_key.bias transformer.9.attention.pattern.project_query.bias\n",
      "transformer.9.attention.pattern.project_key.weight layers.9.layers.0.m.project_value.weight transformer.9.attention.pattern.project_key.weight\n",
      "transformer.9.attention.pattern.project_key.bias layers.9.layers.0.m.project_value.bias transformer.9.attention.pattern.project_key.bias\n",
      "transformer.9.attention.project_value.weight layers.9.layers.0.m.project_output.weight transformer.9.attention.project_value.weight\n",
      "transformer.9.attention.project_value.bias layers.9.layers.0.m.project_output.bias transformer.9.attention.project_value.bias\n",
      "transformer.9.attention.project_out.weight layers.9.layers.1.weight transformer.9.attention.project_out.weight\n",
      "transformer.9.attention.project_out.bias layers.9.layers.1.bias transformer.9.attention.project_out.bias\n",
      "transformer.9.residual.mlp1.weight layers.9.layers.2.m.0.linear1.weight transformer.9.residual.mlp1.weight\n",
      "transformer.9.residual.mlp1.bias layers.9.layers.2.m.0.linear1.bias transformer.9.residual.mlp1.bias\n",
      "transformer.9.residual.mlp2.weight layers.9.layers.2.m.0.linear2.weight transformer.9.residual.mlp2.weight\n",
      "transformer.9.residual.mlp2.bias layers.9.layers.2.m.0.linear2.bias transformer.9.residual.mlp2.bias\n",
      "transformer.9.residual.layer_norm.weight layers.9.layers.3.weight transformer.9.residual.layer_norm.weight\n",
      "transformer.9.residual.layer_norm.bias layers.9.layers.3.bias transformer.9.residual.layer_norm.bias\n",
      "transformer.10.layer_norm.weight layers.10.layers.0.m.project_query.weight transformer.10.layer_norm.weight\n",
      "transformer.10.layer_norm.bias layers.10.layers.0.m.project_query.bias transformer.10.layer_norm.bias\n",
      "transformer.10.attention.pattern.project_query.weight layers.10.layers.0.m.project_key.weight transformer.10.attention.pattern.project_query.weight\n",
      "transformer.10.attention.pattern.project_query.bias layers.10.layers.0.m.project_key.bias transformer.10.attention.pattern.project_query.bias\n",
      "transformer.10.attention.pattern.project_key.weight layers.10.layers.0.m.project_value.weight transformer.10.attention.pattern.project_key.weight\n",
      "transformer.10.attention.pattern.project_key.bias layers.10.layers.0.m.project_value.bias transformer.10.attention.pattern.project_key.bias\n",
      "transformer.10.attention.project_value.weight layers.10.layers.0.m.project_output.weight transformer.10.attention.project_value.weight\n",
      "transformer.10.attention.project_value.bias layers.10.layers.0.m.project_output.bias transformer.10.attention.project_value.bias\n",
      "transformer.10.attention.project_out.weight layers.10.layers.1.weight transformer.10.attention.project_out.weight\n",
      "transformer.10.attention.project_out.bias layers.10.layers.1.bias transformer.10.attention.project_out.bias\n",
      "transformer.10.residual.mlp1.weight layers.10.layers.2.m.0.linear1.weight transformer.10.residual.mlp1.weight\n",
      "transformer.10.residual.mlp1.bias layers.10.layers.2.m.0.linear1.bias transformer.10.residual.mlp1.bias\n",
      "transformer.10.residual.mlp2.weight layers.10.layers.2.m.0.linear2.weight transformer.10.residual.mlp2.weight\n",
      "transformer.10.residual.mlp2.bias layers.10.layers.2.m.0.linear2.bias transformer.10.residual.mlp2.bias\n",
      "transformer.10.residual.layer_norm.weight layers.10.layers.3.weight transformer.10.residual.layer_norm.weight\n",
      "transformer.10.residual.layer_norm.bias layers.10.layers.3.bias transformer.10.residual.layer_norm.bias\n",
      "transformer.11.layer_norm.weight layers.11.layers.0.m.project_query.weight transformer.11.layer_norm.weight\n",
      "transformer.11.layer_norm.bias layers.11.layers.0.m.project_query.bias transformer.11.layer_norm.bias\n",
      "transformer.11.attention.pattern.project_query.weight layers.11.layers.0.m.project_key.weight transformer.11.attention.pattern.project_query.weight\n",
      "transformer.11.attention.pattern.project_query.bias layers.11.layers.0.m.project_key.bias transformer.11.attention.pattern.project_query.bias\n",
      "transformer.11.attention.pattern.project_key.weight layers.11.layers.0.m.project_value.weight transformer.11.attention.pattern.project_key.weight\n",
      "transformer.11.attention.pattern.project_key.bias layers.11.layers.0.m.project_value.bias transformer.11.attention.pattern.project_key.bias\n",
      "transformer.11.attention.project_value.weight layers.11.layers.0.m.project_output.weight transformer.11.attention.project_value.weight\n",
      "transformer.11.attention.project_value.bias layers.11.layers.0.m.project_output.bias transformer.11.attention.project_value.bias\n",
      "transformer.11.attention.project_out.weight layers.11.layers.1.weight transformer.11.attention.project_out.weight\n",
      "transformer.11.attention.project_out.bias layers.11.layers.1.bias transformer.11.attention.project_out.bias\n",
      "transformer.11.residual.mlp1.weight layers.11.layers.2.m.0.linear1.weight transformer.11.residual.mlp1.weight\n",
      "transformer.11.residual.mlp1.bias layers.11.layers.2.m.0.linear1.bias transformer.11.residual.mlp1.bias\n",
      "transformer.11.residual.mlp2.weight layers.11.layers.2.m.0.linear2.weight transformer.11.residual.mlp2.weight\n",
      "transformer.11.residual.mlp2.bias layers.11.layers.2.m.0.linear2.bias transformer.11.residual.mlp2.bias\n",
      "transformer.11.residual.layer_norm.weight layers.11.layers.3.weight transformer.11.residual.layer_norm.weight\n",
      "transformer.11.residual.layer_norm.bias layers.11.layers.3.bias transformer.11.residual.layer_norm.bias\n",
      "lm_head.mlp.weight layers.12.weight lm_head.mlp.weight\n",
      "lm_head.mlp.bias layers.12.bias lm_head.mlp.bias\n",
      "lm_head.unembedding.bias layers.14.weight lm_head.unembedding.bias\n",
      "lm_head.layer_norm.weight layers.14.bias lm_head.layer_norm.weight\n",
      "lm_head.layer_norm.bias layers.15.weight lm_head.layer_norm.bias\n",
      "classification_head.weight layers.15.bias classification_head.weight\n"
     ]
    }
   ],
   "source": [
    "my_bert = Bert(\n",
    "    vocab_size=28996, hidden_size=768, max_position_embeddings=512,\n",
    "    type_vocab_size=2, dropout=0.1, intermediate_size=3072,\n",
    "    num_heads=12, num_layers=12\n",
    ")\n",
    "pretrained_bert = bert_tests.get_pretrained_bert()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 768]) torch.Size([10, 20, 768])\n",
      "torch.Size([10, 20, 768]) torch.Size([10, 20, 768])\n",
      "torch.Size([10, 20, 768]) torch.Size([10, 20, 768])\n",
      "torch.Size([10, 20, 768]) torch.Size([10, 20, 768])\n",
      "torch.Size([10, 20, 768]) torch.Size([10, 20, 768])\n",
      "torch.Size([10, 20, 768]) torch.Size([10, 20, 768])\n",
      "torch.Size([10, 20, 768]) torch.Size([10, 20, 768])\n",
      "torch.Size([10, 20, 768]) torch.Size([10, 20, 768])\n",
      "torch.Size([10, 20, 768]) torch.Size([10, 20, 768])\n",
      "torch.Size([10, 20, 768]) torch.Size([10, 20, 768])\n",
      "torch.Size([10, 20, 768]) torch.Size([10, 20, 768])\n",
      "torch.Size([10, 20, 768]) torch.Size([10, 20, 768])\n",
      "torch.Size([10, 20, 768]) torch.Size([10, 20, 28996])\n",
      "comparing Berts MATCH!!!!!!!!\n",
      " SHAPE (10, 20, 28996) MEAN: -2.796 STD: 2.486 VALS [-4.243 -4.423 -4.44 -4.375 -4.438 -4.2 -4.534 -4.422 -4.317 -4.508...]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def convert(s):\n",
    "    # embedding blocks\n",
    "    key = re.sub(r\"^embedding\", r\"embed\", s)\n",
    "    key = re.sub(r\"^embed.(\\w+_embedding).weight\", r\"embed.\\1.embedding\", key)\n",
    "    key = re.sub(r\"^embed.layer_norm.(\\w+)$\", r\"embed.layernorm.\\1\", key)\n",
    "\n",
    "    # layers\n",
    "    key = re.sub(r\"^transformer.(\\d+)\", r\"layers.\\1\", key)\n",
    "    key = re.sub(r\"^layers.(\\d+).layer_norm\", r\"layers.\\1.layers.1\", key)\n",
    "    key = re.sub(r\"^layers.(\\d+).attention(.pattern)?\", r\"layers.\\1.layers.0.m\", key)\n",
    "    key = re.sub(r\"^layers.(\\d+).residual.mlp(\\d+)\", r\"layers.\\1.layers.2.m.0.linear\\2\", key)\n",
    "    key = re.sub(r\"^layers.(\\d+).residual.layer_norm\", r\"layers.\\1.layers.3\", key)\n",
    "    key = re.sub(r\"project_out\", r\"project_output\", key)\n",
    "\n",
    "    # end bit\n",
    "    key = re.sub(r\"^lm_head.mlp\", r\"layers.12\", key)\n",
    "    key = re.sub(r\"^lm_head.unembedding\", r\"layers.15\", key)\n",
    "    key = re.sub(r\"^lm_head.layer_norm\", r\"layers.14\", key)\n",
    "    return key\n",
    "\n",
    "for (n1, p1), (n2, p2) in zip(pretrained_bert.named_parameters(), my_bert.named_parameters()):\n",
    "    if n1.startswith('lm_head.unembedding.weight'):\n",
    "        print(n1, n2, convert(n1))\n",
    "\n",
    "mapped_params = {convert(k): v for k, v in pretrained_bert.state_dict().items()\n",
    "                 if not k.startswith('classification_head')}\n",
    "my_bert.load_state_dict(mapped_params)\n",
    "bert_tests.test_same_output(my_bert, pretrained_bert, tol=0.3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}